{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d43c3d-6755-4d5f-9d96-df69e2eaa6d5",
   "metadata": {},
   "source": [
    "# Wilk's Theorem\n",
    "\n",
    "Says the following: If you have a pdf $p(x|\\mu)$ where $\\mu$ is (potentially nuissance) aparameter, you can compute $\\lambda = \\frac{p(x|\\mu)}{p(x, \\hat{\\mu})}$ where $p(x|\\hat{\\mu})$ is the profiled likelihood and $\\hat{\\mu}$ is the MLE of $\\mu$, then you compute $t = -2 \\log \\frac{p(x|\\mu)}{p(x, \\hat{\\mu})}$ then as the number of samples $n \\rightarrow \\infty$, $t$ will be distributed according to a $\\chi^2$ distribution, $t \\~\\chi^2_k$ where $k$ is the number of parameters that are left in $p(x|\\hat{\\mu})$.\n",
    "\n",
    "Fisher and others say that the MLE estimate $\\hat{\\theta}_0(x)$ which estimates the true parameter values $\\theta_0$ as a function of the data $x$ is a consistent estimator, that is $\\hat{\\theta}_0 \\rightarrow \\theta_0$ as the number of samples approaches infinity. This is a very important result, implemented by eg. Minuit, because it guarantees that it reaches the true parameter with more data. another way of saying it is that the estimator will be biased, $E[\\hat{\\theta}_0] = \\theta_0 + b(x)$ where $b(x)$ is the bias, and the MLE consistency states that $b(x) \\rightarrow 0$ as $n \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4dfab5-b2a4-4e5a-af60-cdba3ac2d216",
   "metadata": {},
   "source": [
    "When you have a model which becomes the likelihood of two parameters $P(X|\\theta_1, \\theta_2) =L(x|\\theta_1, \\theta_2)$ after you plug in the date ($X$ becomes observed data $x$), then you compute the profile likilihood, which will be a function of both, $L(x|\\hat{\\theta_1}, \\hat{\\theta_2})$, you can plot it in the $(\\theta_1, \\theta_2)$ plane, forming contours.\n",
    "\n",
    "Consistency: since we said that the estimator converges to the true parameter, we have the following implication. Say the probability model \n",
    "\n",
    "$$P(X|\\theta) = \\prod_{i=1}^n f(x_i|\\theta)$$,\n",
    "\n",
    "which says that the model is a product of each point which is distributed along the same PDF (say a gaussian), then the natural thing to do is to do is to take the log of both sides\n",
    "\n",
    "$$ log P(X|\\theta) = \\sum_{i=1}^n \\log f(x_i |\\theta)$$\n",
    "\n",
    "and if we take the average of this $ \\frac{1}{n} \\sum_{i=1}^n \\log f(x_i |\\theta)$ then this should converge to the true expectation value, i.e.\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{i=1}^n \\log f(x_i |\\theta) \\rightarrow \\int [\\log f(X|\\theta)] f(x|\\theta_0) dx= E[\\log f(x|\\theta)] $$\n",
    "\n",
    "Where $\\theta_0$ is the true parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147bc78d-5da2-4dab-94ef-7517dfcc416e",
   "metadata": {},
   "source": [
    "## Hypothesis Tests\n",
    "\n",
    "The kind of hypothesis tests that are relevent to physics are the existance of a null hypothesis $H_0: s=0$ as an example this null hypothesis says there is no signal, meaning no new physics, everything is background that is accounted for in the SM, and an laternaltive hypothesis $H_1: s=2.5$, where in this example this hypothesis predicts 2.5 counts on average for the signal.\n",
    "\n",
    "# Neyman-Pearson vs Fisher\n",
    "We need two thigs according to N-P, a null hypothesis $H_0$ and an alternative $H_1$, and we need a test statistic $T$ (that you choose), which is a function of the observation. $T$ resides in a space $\\Omega$, $T \\in \\Omega$. The purpose of $T$ is to partition the space $\\Omega$ into a critical region $\\omega$ and its compliment, $\\Omega-\\omega$, such that if an observed instance of $T$, say $t (x) \\in T(X)$ lies in $\\omega$, the null hypothesis $H_0$ is rejected.\n",
    "\n",
    "The probability that $T$ lies in $\\omega$, and $H_0$ is in fact true, if $\\alpha$\n",
    "\n",
    "$$P( T \\in \\omega |H_0) = \\alpha$$\n",
    "\n",
    "Which is the probability pf making a type I error, $\\alpha = P(\\text{Type I error})$ (it's an error because you are going to be rejecting $H_0$ when it is in fact true. $\\alpha$ is called the significance of the test. You don't want to make such an error, so you set to be very low. In the medical field, they choose $\\alpha=0.05$, which is a weak criterion, whereas in particle physics we choose $\\alpha \\approx 2.7 \\times 10^{-7}$, so we have more stringent requirements.\n",
    "\n",
    "If $H_1$ is in fact true, and we reject it, we are also making an error. \n",
    "\n",
    "$$P(T \\in (\\Omega-\\omega)|H_1) = \\beta$$\n",
    "\n",
    "$$\\text{Power} = 1-\\beta $$\n",
    "and we want the power to be as high as possible.\n",
    "\n",
    "The point of the NP test, is that after you choose $\\alpha$ to be small, you want to choose $T$ that gives you the smallest $\\beta$, ie the one that maximizes the power.\n",
    "\n",
    "In Fisher's approach, he doesn't care about $H_1$, all we have is $H_0$, so what is needed is simply to compute the p-value\n",
    "\n",
    "$$p-\\text{value} = \\int_t^\\infty P(T|H_0) dT$$\n",
    "\n",
    "Where $t$ is the observed value of $T$, and you want $p$ as small as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d03cc1-b358-43f7-9e6b-9c61806d92dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
