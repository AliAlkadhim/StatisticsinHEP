{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d43c3d-6755-4d5f-9d96-df69e2eaa6d5",
   "metadata": {},
   "source": [
    "# Wilk's Theorem\n",
    "\n",
    "Says the following: If you have a pdf $p(x|\\mu)$ where $\\mu$ is (potentially nuissance) aparameter, you can compute $\\lambda = \\frac{p(x|\\mu)}{p(x, \\hat{\\mu})}$ where $p(x|\\hat{\\mu})$ is the profiled likelihood and $\\hat{\\mu}$ is the MLE of $\\mu$, then you compute \n",
    "$$t = -2 \\log \\frac{p(x|\\mu)}{p(x, \\hat{\\mu})}$$\n",
    "\n",
    "then as the number of samples $n \\rightarrow \\infty$, $t$ will be distributed according to a $\\chi^2$ distribution, $t \\~\\chi^2_k$ where $k$ is the number of parameters that are left in $p(x|\\hat{\\mu})$.\n",
    "\n",
    "Fisher and others say that the MLE estimate $\\hat{\\theta}_0(x)$ which estimates the true parameter values $\\theta_0$ as a function of the data $x$ is a consistent estimator, that is $\\hat{\\theta}_0 \\rightarrow \\theta_0$ as the number of samples approaches infinity. This is a very important result, implemented by eg. Minuit, because it guarantees that it reaches the true parameter with more data. another way of saying it is that the estimator will be biased, $E[\\hat{\\theta}_0] = \\theta_0 + b(x)$ where $b(x)$ is the bias, and the MLE consistency states that $b(x) \\rightarrow 0$ as $n \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4dfab5-b2a4-4e5a-af60-cdba3ac2d216",
   "metadata": {},
   "source": [
    "When you have a model which becomes the likelihood of two parameters $P(X|\\theta_1, \\theta_2) =L(x|\\theta_1, \\theta_2)$ after you plug in the date ($X$ becomes observed data $x$), then you compute the profile likilihood, which will be a function of both, $L(x|\\hat{\\theta_1}, \\hat{\\theta_2})$, you can plot it in the $(\\theta_1, \\theta_2)$ plane, forming contours.\n",
    "\n",
    "Consistency: since we said that the estimator converges to the true parameter, we have the following implication. Say the probability model \n",
    "\n",
    "$$P(X|\\theta) = \\prod_{i=1}^n f(x_i|\\theta)$$,\n",
    "\n",
    "which says that the model is a product of each point which is distributed along the same PDF (say a gaussian), then the natural thing to do is to do is to take the log of both sides\n",
    "\n",
    "$$ log P(X|\\theta) = \\sum_{i=1}^n \\log f(x_i |\\theta)$$\n",
    "\n",
    "and if we take the average of this $ \\frac{1}{n} \\sum_{i=1}^n \\log f(x_i |\\theta)$ then this should converge to the true expectation value, i.e.\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{i=1}^n \\log f(x_i |\\theta) \\rightarrow \\int [\\log f(X|\\theta)] f(x|\\theta_0) dx= E[\\log f(x|\\theta)] $$\n",
    "\n",
    "Where $\\theta_0$ is the true parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147bc78d-5da2-4dab-94ef-7517dfcc416e",
   "metadata": {},
   "source": [
    "## Hypothesis Tests\n",
    "\n",
    "The kind of hypothesis tests that are relevent to physics are the existance of a null hypothesis $H_0: s=0$ as an example this null hypothesis says there is no signal, meaning no new physics, everything is background that is accounted for in the SM, and an laternaltive hypothesis $H_1: s=2.5$, where in this example this hypothesis predicts 2.5 counts on average for the signal.\n",
    "\n",
    "# Neyman-Pearson vs Fisher\n",
    "We need two thigs according to N-P, a null hypothesis $H_0$ and an alternative $H_1$, and we need a test statistic $T$ (that you choose), which is a function of the observation. $T$ resides in a space $\\Omega$, $T \\in \\Omega$. The purpose of $T$ is to partition the space $\\Omega$ into a critical region $\\omega$ and its compliment, $\\Omega-\\omega$, such that if an observed instance of $T$, say $t (x) \\in T(X)$ lies in $\\omega$, the null hypothesis $H_0$ is rejected.\n",
    "\n",
    "The probability that $T$ lies in $\\omega$, and $H_0$ is in fact true, if $\\alpha$\n",
    "\n",
    "$$P( T \\in \\omega |H_0) = \\alpha$$\n",
    "\n",
    "Which is the probability pf making a type I error, $\\alpha = P(\\text{Type I error})$ (it's an error because you are going to be rejecting $H_0$ when it is in fact true. Note here that this assumes that the hypotheses requirement (say that $H_0$ is true implies that the hypothesis have to be \\emph{perfectly specified}, ie there is no uncertainty in the parameter for which the hypothessi is specified. $\\alpha$ is called the significance of the test. You don't want to make such an error, so you set to be very low. In the medical field, they choose $\\alpha=0.05$, which is a weak criterion, whereas in particle physics we choose $\\alpha \\approx 2.7 \\times 10^{-7}$, so we have more stringent requirements.\n",
    "\n",
    "If $H_1$ is in fact true, and we reject it, we are also making an error. \n",
    "\n",
    "$$P(T \\in (\\Omega-\\omega)|H_1) = \\beta$$\n",
    "\n",
    "$$\\text{Power} = 1-\\beta $$\n",
    "and we want the power to be as high as possible.\n",
    "\n",
    "The point of the NP test, is that after you choose $\\alpha$ to be small, you want to choose $T$ that gives you the smallest $\\beta$, ie the one that maximizes the power (power curve) via something like \n",
    "$$\\frac{p\\left(T \\mid H_{0}\\right)}{p\\left(T \\mid H_{1}\\right)} \\leq C(\\alpha)$$\n",
    "\n",
    "where $\\alpha$ is fixed from $P( T \\in \\omega |H_0) = \\alpha$. The general algorithm for constructing the critical region is however\n",
    "$$ T(x) = \\frac{p(x|H_1)}{p(x|H_0)} > C(\\alpha)$$\n",
    "\n",
    "where the ratio is referred to as $T(x)$, the test statistic. This analogous to the Felman-Cousins method, where the steps are:\n",
    "* compute $T(x)$ for \\emph{all} possible values of $x$.\n",
    "* Order these $T(x)$ in some wy: $T_1 >T_2>//$ and find the threshold $C+T$ for which $P(T>C|H_0) \\le \\alpha$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-------------- \n",
    "# EXAMPLE: HEP analysis\n",
    "\n",
    "ATLAS collaboration reported the results for $pp \\rightarrow W^{\\pm} W^{\\pm} jj$ where $W^\\pm$ decay to muons, the results were \n",
    "* $\\boldsymbol{N}=10$ observed events\n",
    "* $\\boldsymbol{B}=3 \\pm 0.6$ background events\n",
    "*$\\boldsymbol{S}=9.3 \\pm 1.0$ predicted signal events\n",
    "\n",
    "Bold for \"observed\". If we were to use NP theory, we have to neglect the uncertainies, so that $B=3, S=9.3$. The we can take the probability model to be \n",
    "\n",
    "$$p(\\boldsymbol{x}|s) = \\frac{e^{-(s+b)} (s+b)^\\boldsymbol{x}}{\\boldsymbol{x}!}$$\n",
    "with $b=\\boldsymbol{B}$. Let's consider two hypotheses. \n",
    "$$\\begin{array}{l}\n",
    "H_{0}: s=0 \\\\\n",
    "H_{1}: s=9.3\n",
    "\\end{array}$$\n",
    "\n",
    "\n",
    "Let's take the test statistic $T(x)$ to be \n",
    "$$ T(x) = \\frac{P(x|H_1)}{P(x|H_0)} = \\frac{\\text{Poiss}(x, s+b)}{\\text{Poiss}(x, b)}= e^{-s}( \\frac{s}{b}+1)^x$$\n",
    "And we choose this so that if there were large values of signal it would favor the numerator. Let's arbitrarily choose the level of significance $\\alpha=0.01$ and we wnat to make a decision whether or not to reject the null hypothesis.\n",
    "\n",
    "$$P(x \\ge x_\\alpha|H_0) =\\sum_{x=x_\\alpha}^\\infty P(x|H_0) = \\sum_{x=x_\\alpha}^\\infty e^{-b} b^x/{x!} \\le \\alpha$$\n",
    "\n",
    "Therefore we want to find this threshold $x_\\alpha$ which corresponds to presumably $c=T(x_\\alpha)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d03cc1-b358-43f7-9e6b-9c61806d92dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12508e0b-da37-48f9-a215-7f5344d09c39",
   "metadata": {},
   "source": [
    "----------------\n",
    "# FIsher\n",
    "In Fisher's approach, he doesn't care about $H_1$, all we have is $H_0$, so what is needed is simply to compute the p-value\n",
    "\n",
    "$$p-\\text{value} = \\int_t^\\infty P(T|H_0) dT$$\n",
    "\n",
    "Where $t$ is the observed value of $T$, and you want $p$ as small as possible. Of course in HEP we use both NP and Fisher's methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3cb89-0db4-4388-90cd-65b7d1a8672e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
